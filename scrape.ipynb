{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "proxies = {\n",
    "    'http': 'http://26.26.26.1:10809',\n",
    "    'https': 'http://26.26.26.1:10809',\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collecting\n",
    "\n",
    "We will use the `requests` library to get the data from the website. We will use the `BeautifulSoup` library to parse the data and `pandas` library to store the data in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url of the webpage to scrape\n",
    "url = 'https://www.worldometers.info/coronavirus/'\n",
    "\n",
    "# using requests to get the webpage with proxy\n",
    "webpage = requests.get(url, proxies=proxies)\n",
    "\n",
    "# using requests to get the webpage without proxy\n",
    "# webpage = requests.get(url)\n",
    "\n",
    "# using BeautifulSoup to parse the webpage\n",
    "soup = BeautifulSoup(webpage.text, 'lxml')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the table we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_delta = 0 is today\n",
    "# date_delta = -1 is yesterday\n",
    "# date_delta = -2 is 2 days ago\n",
    "date_delta = 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get header of the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all table tag in the webpage\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# find all th tags in the table\n",
    "table_header = tables[abs(date_delta)].find_all('th')\n",
    "\n",
    "header = []\n",
    "for i in range(16):\n",
    "    # append the header to the list and format it\n",
    "    header.append(table_header[i].text.replace('\\n', '').replace('\\xa0', ''))\n",
    "\n",
    "# remove the first element \"#\" in the header which is not needed\n",
    "header.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country,Other',\n",
       " 'TotalCases',\n",
       " 'NewCases',\n",
       " 'TotalDeaths',\n",
       " 'NewDeaths',\n",
       " 'TotalRecovered',\n",
       " 'NewRecovered',\n",
       " 'ActiveCases',\n",
       " 'Serious,Critical',\n",
       " 'TotCases/1M pop',\n",
       " 'Deaths/1M pop',\n",
       " 'TotalTests',\n",
       " 'Tests/1M pop',\n",
       " 'Population',\n",
       " 'Continent']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "covid_data = pd.DataFrame(columns=header)\n",
    "\n",
    "# find all rows in the table\n",
    "for row in tables[abs(date_delta)].tbody.find_all('tr'):\n",
    "    # find all columns in the row\n",
    "    col = row.find_all('td')\n",
    "    if (col != []):\n",
    "        # col[0] is index which is not needed\n",
    "        country = col[1].text.strip()\n",
    "        total_cases = col[2].text.strip()\n",
    "        new_cases = col[3].text.strip()\n",
    "        total_deaths = col[4].text.strip()\n",
    "        new_deaths = col[5].text.strip()\n",
    "        total_recovered = col[6].text.strip()\n",
    "        new_recovered = col[7].text.strip()\n",
    "        active_cases = col[8].text.strip()\n",
    "        serious = col[9].text.strip()\n",
    "        total_cases_per_m = col[10].text.strip()\n",
    "        deaths = col[11].text.strip()\n",
    "        total_tests = col[12].text.strip()\n",
    "        tests_per_m = col[13].text.strip()\n",
    "        population = col[14].text.strip()\n",
    "        continent = col[15].text.strip()\n",
    "        # append the data to the DataFrame\n",
    "        covid_data = covid_data.append({\"Country,Other\": country, \"TotalCases\": total_cases, \"NewCases\": new_cases,\n",
    "                                        \"TotalDeaths\": total_deaths, \"NewDeaths\": new_deaths, \"TotalRecovered\": total_recovered,\n",
    "                                        \"NewRecovered\": new_recovered, \"ActiveCases\": active_cases, \"Serious,Critical\": serious,\n",
    "                                        \"TotCases/1M pop\": total_cases_per_m, \"Deaths/1M pop\": deaths, \"TotalTests\": total_tests,\n",
    "                                        \"Tests/1M pop\": tests_per_m, \"Population\": population, \"Continent\": continent}, ignore_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the date of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the date of the data\n",
    "now = datetime.now()\n",
    "\n",
    "# if date_delta is -1, get yesterday date\n",
    "# if date_delta is -2, get 2 days ago date\n",
    "now = now - timedelta(abs(date_delta))\n",
    "\n",
    "# format the date\n",
    "date = now.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-03-06'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data we just scraped into a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data to CSV file\n",
    "covid_data.to_csv(f\"data/{date}.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "https://github.com/Shaikhmohddanish/Covid-data-webscraping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
